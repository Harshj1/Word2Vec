{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshj1/Word2Vec/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4LyXGs8vk7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zchX5-gNuhRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"natural language processing and machine learning is fun and exciting\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eHs10h4vB08",
        "colab_type": "code",
        "outputId": "3915e256-33dc-4a80-aeef-d42e03e1cf08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "corpus = [[word.lower() for word in text.split()]]\n",
        "corpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'and',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'is',\n",
              "  'fun',\n",
              "  'and',\n",
              "  'exciting']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EYvYGiLvgBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings = {\n",
        "\t'window_size': 2,\t# context window +- center word\n",
        "\t'n': 10,\t\t# dimensions of word embeddings, also refer to size of hidden layer\n",
        "\t'epochs': 50,\t\t# number of training epochs\n",
        "\t'learning_rate': 0.01\t# learning rate\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSUEivym5tMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class word2vec():\n",
        "  def __init__(self):\n",
        "    self.n = settings['n']\n",
        "    self.lr = settings['learning_rate']\n",
        "    self.epochs = settings['epochs']\n",
        "    self.window = settings['window_size']\n",
        "\n",
        "  def generate_training_data(self, settings, corpus):\n",
        "    word_counts = defaultdict(int)\n",
        "    for row in corpus:\n",
        "      for word in row:\n",
        "        word_counts[word] += 1\n",
        "    self.v_count = len(word_counts.keys())\n",
        "    self.words_list = list(word_counts.keys())\n",
        "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
        "\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "      sent_len = len(sentence)\n",
        "      for i, word in enumerate(sentence):\n",
        "        w_target = self.word2onehot(sentence[i])\n",
        "        w_context = []\n",
        "        for j in range(i - self.window, i + self.window+1):\n",
        "          if j != i and j <= sent_len-1 and j >= 0:\n",
        "            w_context.append(self.word2onehot(sentence[j]))\n",
        "        training_data.append([w_target, w_context])\n",
        "    return np.array(training_data)\n",
        "\n",
        "  def word2onehot(self, word):\n",
        "    word_vec = np.zeros(self.v_count)\n",
        "    word_index = self.word_index[word]\n",
        "    word_vec[word_index] = 1\n",
        "    return word_vec\n",
        "\n",
        "  def train(self, training_data):\n",
        "    self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
        "    self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      # Intialise loss to 0\n",
        "      self.loss = 0\n",
        "\n",
        "      for w_t, w_c in training_data:\n",
        "        y_pred, h, u = self.forward_pass(w_t)\n",
        "        \n",
        "  def forward_pass(self, x):\n",
        "    h = np.dot(self.w1.transpose(), x)\n",
        "    u = np.dot(self.w2.transpose(), h)\n",
        "    y_c = self.softmax(u)\n",
        "    return y_c, h, u\n",
        "  \n",
        "  def softmax(self, x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ0o70PPtjuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise object\n",
        "w2v = word2vec()\n",
        "training_data = w2v.generate_training_data(settings, corpus)\n",
        "w2v.train(training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYCFD1_tlay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}